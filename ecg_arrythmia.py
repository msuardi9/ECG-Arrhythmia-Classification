# -*- coding: utf-8 -*-
"""ECG_arrythmia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PhCLvSjuuFdVEEQqrkNlCw_8NYYReqaa
"""

from google.colab import drive
drive.mount('/content/drive')

import kagglehub

path = kagglehub.dataset_download("shayanfazeli/heartbeat")

print("Path to dataset files:", path)

import os

folder = "/content/drive/MyDrive/ECG Arrhythmia Classification/archive 2"

import pandas as pd

mitbih_train = pd.read_csv(os.path.join(folder, "mitbih_train.csv"), header=None)
mitbih_test = pd.read_csv(os.path.join(folder, "mitbih_test.csv"), header=None)

ptb_normal = pd.read_csv(os.path.join(folder, "ptbdb_normal.csv"), header=None)
ptb_abnormal = pd.read_csv(os.path.join(folder, "ptbdb_abnormal.csv"), header=None)

ptb_full = pd.concat([ptb_normal, ptb_abnormal], ignore_index=True)
ptb_full = ptb_full.sample(frac=1, random_state=42).reset_index(drop=True)

from sklearn.model_selection import train_test_split

X_ptb = ptb_full.iloc[:, :-1].values
y_ptb = ptb_full.iloc[:, -1].values

X_ptb_train, X_ptb_test, y_ptb_train, y_ptb_test = train_test_split(X_ptb, y_ptb, test_size=0.2, stratify=y_ptb, random_state=42)

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from tensorflow.keras.utils import to_categorical
from sklearn.utils import class_weight
import warnings
warnings.filterwarnings('ignore')

"""# Class balance"""

print("MIT-BIH TRAIN:")
print(mitbih_train[187].value_counts(normalize=True)*100)

print("\nMIT-BIH TEST:")
print(mitbih_test[187].value_counts(normalize=True)*100)

print("PTB TRAIN:")
print(pd.Series(y_ptb_train).value_counts(normalize=True)*100)

print("\nPTB TEST:")
print(pd.Series(y_ptb_test).value_counts(normalize=True)*100)

import matplotlib.pyplot as plt
import pandas as pd

mitbih_train_dist = mitbih_train[187].value_counts(normalize=True) * 100
mitbih_test_dist = mitbih_test[187].value_counts(normalize=True) * 100
ptb_train_dist = pd.Series(y_ptb_train).value_counts(normalize=True) * 100
ptb_test_dist = pd.Series(y_ptb_test).value_counts(normalize=True) * 100

labels_mitbih = ['N (0)', 'S (1)', 'V (2)', 'F (3)', 'Q (4)']
colors_mitbih = ['red', 'skyblue', 'blue', 'orange', 'green']

labels_ptb = ['Normal (0)', 'Myocardial Infarction (1)']
colors_ptb = ['limegreen', 'crimson']

def plot_donut(data, labels, colors, title):
    plt.figure(figsize=(5.5,5.5))
    my_circle = plt.Circle((0,0), 0.70, color='white')
    plt.pie(data, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    p = plt.gcf()
    p.gca().add_artist(my_circle)
    plt.title(title)
    plt.axis('equal')
    plt.show()

plot_donut(mitbih_train_dist.sort_index(), labels_mitbih, colors_mitbih, "MIT-BIH Train Set")

plot_donut(mitbih_test_dist.sort_index(), labels_mitbih, colors_mitbih, "MIT-BIH Test Set")

plot_donut(ptb_train_dist.sort_index(), labels_ptb, colors_ptb, "PTB Train Set")

plot_donut(ptb_test_dist.sort_index(), labels_ptb, colors_ptb, "PTB Test Set")

"""Classes:

- **N(0)**: Non-ecotic beat (normal beat): battito normale (82.8%).
- **S(1)**: Superventricular ectopic beat: battito ectopico sopraventricolare (origine atriale o congiunzionale). Può indicare **aritmie benigne** o **tachicardie sopraventricolari** (2.5%).
- **V(2)**: Ventriculatr ectopic beat: battito ectopico ventricolare (origine anomala nei ventricoli). Potenzialmente **pericoloso**, associato ad **aritmie gravi** (6.6%).
- **F(3)**: Fusion beat: **fusione** tra battito normale e ventricolare. Evento raro, indicativo di **conduzione anomala** (0.7%).
- **Q(4)**: Unknown beat: battito **sconosciuto** o rumoroso, difficile da classificare automaticamente. Può derivare da **artefatti, errori o casi ambigui** (7.3%).

## Preprocessing and class balancing
"""

from sklearn.utils import resample

dfs = []
for label in mitbih_train[187].unique():
  class_df = mitbih_train[mitbih_train[187] == label]

  if label == 0:
    class_df = resample(class_df, replace=True, n_samples=10000, random_state=42)
  else:
    class_df = resample(class_df, replace=True, n_samples=10000, random_state=42)
  dfs.append(class_df)

balanced_df = pd.concat(dfs).sample(frac=1, random_state=42)

X = balanced_df.iloc[:, :-1].values
y = balanced_df.iloc[:, -1].values.astype(int)

from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

scaler = StandardScaler()
X = scaler.fit_transform(X)

X = X.reshape((X.shape[0], X.shape[1], 1))

y_cat = to_categorical(y, num_classes=5)

X_train, X_val, y_train, y_val = train_test_split(X, y_cat, test_size=0.2, stratify=y, random_state=42)

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x=y)
plt.title("Class distribution on balanced dataset")
plt.xlabel("Class")
plt.ylabel("Samples")
plt.show()

for i in range(5):
    plt.plot(X[i].squeeze(), label=f"Class: {y[i]}")
    plt.title(f"ECG segment {i+1} - Class {y[i]}")
    plt.xlabel("Samples")
    plt.ylabel("Normalized width")
    plt.grid(True)
    plt.show()

X_raw = balanced_df.iloc[:, :-1].values  # dati originali
X_norm = scaler.fit_transform(X_raw)     # normalizzati

plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(X_raw[0])
plt.title("Before normalization")

plt.subplot(1,2,2)
plt.plot(X_norm[0])
plt.title("After normalization (Z-score)")
plt.show()

"""# CNN definition

# CNN
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

def augment_ecg(ecg_signal):
    noise = np.random.normal(0, 0.02, ecg_signal.shape)
    return ecg_signal + noise

X_train_aug = np.array([augment_ecg(x) for x in X_train])
X_train_combined = np.concatenate((X_train, X_train_aug))
y_train_combined = np.concatenate((y_train, y_train))  # stesso label

model2 = Sequential([
    Conv1D(64, 5, activation='relu', kernel_regularizer=l2(0.001), input_shape=(187, 1)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.25),

    Conv1D(128, 3, activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.25),

    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(5, activation='softmax')  # 5 classi MIT-BIH
])

model2.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

checkpoint_path = "/content/drive/MyDrive/ECG Arrhythmia Classification/best_model.keras"

callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),
    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)
]

history2 = model2.fit(
    X_train_combined, y_train_combined,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=128,
    callbacks=callbacks,
    verbose=1
)

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history2.history['accuracy'], label='Train')
plt.plot(history2.history['val_accuracy'], label='Validation')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history2.history['loss'], label='Train')
plt.plot(history2.history['val_loss'], label='Validation')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

y_pred2 = model2.predict(X_val)
y_pred_classes2 = np.argmax(y_pred2, axis=1)
y_true2 = np.argmax(y_val, axis=1)

print(classification_report(y_true2, y_pred_classes2, digits=4))

cm2 = confusion_matrix(y_true2, y_pred_classes2)
plt.figure(figsize=(6, 5))
sns.heatmap(cm2, annot=True, fmt="d", cmap="Blues", xticklabels=[0, 1, 2, 3, 4], yticklabels=[0, 1, 2, 3, 4])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix (Model 2)")
plt.show()

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from itertools import cycle
import matplotlib.pyplot as plt
import numpy as np

n_classes = 5
class_names = ['N (0)', 'S (1)', 'V (2)', 'F (3)', 'Q (4)']

y_score = model2.predict(X_val)

fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_val[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

fpr["micro"], tpr["micro"], _ = roc_curve(y_val.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

plt.figure(figsize=(10, 7))

colors = cycle(['red', 'blue', 'green', 'orange', 'purple'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC {class_names[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.plot(fpr["micro"], tpr["micro"], color='black', linestyle='--',
         label=f'Micro-average ROC (AUC = {roc_auc["micro"]:.2f})', lw=2)

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Model 2 (Multiclass)')
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

print("Sample prediction:", y_score[0])
print("Argmax class:", np.argmax(y_score[0]))
print("One-hot true label:", y_val[0])

"""## Tuning second model"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, average_precision_score
from sklearn.preprocessing import label_binarize

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

num_classes = len(np.unique(y))
y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)
y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)

model = models.Sequential([
    layers.Input(shape=(X.shape[1], 1)),
    layers.Conv1D(32, kernel_size=5, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.MaxPooling1D(pool_size=2),
    layers.Dropout(0.3),

    layers.Conv1D(64, kernel_size=5, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.MaxPooling1D(pool_size=2),
    layers.Dropout(0.3),

    layers.Flatten(),
    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dropout(0.5),

    layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

checkpoint_path = "/content/drive/MyDrive/ECG Arrhythmia Classification/best_model.keras"

callbacks = [
    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),
    ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)
]

history = model.fit(
    X_train[..., np.newaxis], y_train_cat,
    validation_data=(X_val[..., np.newaxis], y_val_cat),
    epochs=100,
    batch_size=128,
    callbacks=callbacks,
    verbose=1
)

plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

y_pred_probs = model.predict(X_val[..., np.newaxis])
y_pred_classes = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_val_cat, axis=1)

print(classification_report(y_true, y_pred_classes, digits=4))

cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(num_classes), yticklabels=range(num_classes))
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

y_score = y_pred_probs
y_true_bin = label_binarize(y_true, classes=np.arange(num_classes))

plt.figure(figsize=(8, 6))
for i in range(num_classes):
    precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_score[:, i])
    ap_score = average_precision_score(y_true_bin[:, i], y_score[:, i])
    plt.plot(recall, precision, label=f"Class {i} (AP = {ap_score:.2f})")

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve (Multiclass)')
plt.legend(loc='lower left')
plt.grid(True)
plt.show()

"""# Test su dati fuori distribuzione

Calcolo funzioni di disturbo
"""

def add_gaussian_noise(X, std=0.1):
    return X + np.random.normal(0, std, X.shape)

def baseline_shift(X, shift=0.3):
    return X + shift

def amplitude_scaling(X, factor=1.2):
    return X * factor

def random_masking(X, mask_ratio=0.1):
    X_noisy = X.copy()
    n_mask = int(mask_ratio * X.shape[1])
    for i in range(X.shape[0]):
        idx = np.random.choice(X.shape[1], n_mask, replace=False)
        X_noisy[i, idx] = 0
    return X_noisy


def time_warping(X, factor=0.02):

    orig_shape = X.shape
    X_flat = X.reshape(X.shape[0], -1)  # (N, L)
    N, L = X_flat.shape
    orig = np.arange(L)
    X_warped = np.zeros_like(X_flat)

    for i in range(N):
        x = X_flat[i]

        noise = np.random.normal(0, factor * L, size=L)
        stretched = orig + noise
        stretched = np.clip(stretched, 0, L - 1)
        order = np.argsort(stretched)
        xp = stretched[order]
        fp = x[order]
        X_warped[i] = np.interp(orig, xp, fp)

    return X_warped.reshape(orig_shape)

"""Funzione di valutazione della robustezza"""

def evaluate_on_perturbed_data(model, X_val, y_val, transform_fn, label, y_val_cat=None):
    X_perturbed = transform_fn(X_val)
    X_input = X_perturbed[..., np.newaxis]

    loss, acc = model.evaluate(X_input, tf.keras.utils.to_categorical(y_val, num_classes), verbose=0)
    y_probs = model.predict(X_input)
    confidence = np.mean(np.max(y_probs, axis=1))

    print(f"\n{label}:")
    print(f" - Accuracy:  {acc:.4f}")
    print(f" - Loss:      {loss:.4f}")
    print(f" - Avg confidence: {confidence:.4f}")

print("=== Evaluation on clean validation data ===")
evaluate_on_perturbed_data(model, X_val, y_val, lambda x: x, "Original")

print("\n=== Evaluation on perturbed data ===")
evaluate_on_perturbed_data(model, X_val, y_val, add_gaussian_noise, "Gaussian Noise")
evaluate_on_perturbed_data(model, X_val, y_val, baseline_shift, "Baseline Shift")
evaluate_on_perturbed_data(model, X_val, y_val, amplitude_scaling, "Amplitude Scaling")
evaluate_on_perturbed_data(model, X_val, y_val, random_masking, "Random Masking")
evaluate_on_perturbed_data(model, X_val, y_val, time_warping, "Time Warping")

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd

def eval_metrics(model, X, y_cat, perturb_fn=None, name="Original"):
    if perturb_fn is not None:
        X_in = perturb_fn(X.copy())
    else:
        X_in = X
    X_in = X_in[..., np.newaxis]  # aggiunge canale
    probs = model.predict(X_in, verbose=0)
    preds = np.argmax(probs, axis=1)
    truths = np.argmax(y_cat, axis=1)
    acc = (preds == truths).mean()
    loss = tf.keras.losses.categorical_crossentropy(y_cat, probs).numpy().mean()
    conf = probs.max(axis=1).mean()
    return {"Perturbation": name, "Accuracy": acc, "Loss": loss, "AvgConfidence": conf}

results = []

results.append(eval_metrics(model, X_val, y_val_cat, None, "Original"))
results.append(eval_metrics(model, X_val, y_val_cat, add_gaussian_noise, "Gaussian Noise"))
results.append(eval_metrics(model, X_val, y_val_cat, baseline_shift, "Baseline Shift"))
results.append(eval_metrics(model, X_val, y_val_cat, amplitude_scaling, "Amplitude Scaling"))
results.append(eval_metrics(model, X_val, y_val_cat, random_masking, "Random Masking"))
results.append(eval_metrics(model, X_val, y_val_cat, time_warping, "Time Warping"))

df = pd.DataFrame(results).set_index("Perturbation")

fig, ax1 = plt.subplots(figsize=(10,5))
bar_w = 0.35
idx = np.arange(len(df))

ax1.bar(idx - bar_w/2, df["Accuracy"], bar_w, label="Accuracy", color="C0")
ax1.set_ylabel("Accuracy", color="C0"); ax1.set_ylim(0,1); ax1.tick_params(axis='y', labelcolor='C0')

ax2 = ax1.twinx()
ax2.bar(idx + bar_w/2, df["AvgConfidence"], bar_w, label="Avg Confidence", color="C1")
ax2.set_ylabel("Avg Confidence", color="C1"); ax2.set_ylim(0,1); ax2.tick_params(axis='y', labelcolor='C1')

plt.xticks(idx, df.index, rotation=30, ha='right')
plt.title("Accuracy vs Confidence media per perturbazione")
fig.tight_layout()
plt.show()

"""# Confronto con altre architetture"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers, Input, Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
num_classes = len(np.unique(y))
y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)
y_val_cat   = tf.keras.utils.to_categorical(y_val,   num_classes)

X_train_c = X_train[..., np.newaxis]
X_val_c   = X_val[...,   np.newaxis]

print("Train:", X_train_c.shape, y_train_cat.shape)
print("Val:  ", X_val_c.shape,   y_val_cat.shape)

def build_cnn(input_shape, n_classes):
    inp = Input(input_shape)
    x = layers.Conv1D(32, 5, activation='relu', kernel_regularizer=regularizers.l2(1e-3))(inp)
    x = layers.MaxPooling1D(2)(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Conv1D(64, 5, activation='relu', kernel_regularizer=regularizers.l2(1e-3))(x)
    x = layers.MaxPooling1D(2)(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-3))(x)
    x = layers.Dropout(0.5)(x)
    out = layers.Dense(n_classes, activation='softmax')(x)
    return Model(inp, out, name="CNN1D")

def build_gru(input_shape, n_classes):
    inp = Input(input_shape)
    x = layers.GRU(64, return_sequences=True)(inp)
    x = layers.Dropout(0.3)(x)
    x = layers.GRU(64)(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    out = layers.Dense(n_classes, activation='softmax')(x)
    return Model(inp, out, name="GRU1D")

def build_resnet1d(input_shape, n_classes):
    inp = Input(input_shape)
    x = layers.Conv1D(64, 7, padding='same', activation='relu')(inp)

    def res_block(x, filters, kernel):
        y = layers.Conv1D(filters, kernel, padding='same', activation='relu')(x)
        y = layers.Conv1D(filters, kernel, padding='same')(y)
        return layers.Activation('relu')(layers.add([x, y]))
    x = res_block(x, 64, 3)
    x = layers.MaxPooling1D(2)(x)
    x = res_block(x, 64, 3)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dropout(0.5)(x)
    out = layers.Dense(n_classes, activation='softmax')(x)
    return Model(inp, out, name="ResNet1D")

input_shape = (X_train_c.shape[1], 1)
models_dict = {
    "CNN1D":   build_cnn(input_shape, num_classes),
    "GRU1D":   build_gru(input_shape, num_classes),
    "ResNet1D":build_resnet1d(input_shape, num_classes),
}
for name, m in models_dict.items():
    print(m.name, m.count_params())

CALLBACKS_COMMON = [
    EarlyStopping(   monitor='val_loss', patience=10, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),
]
DRIVE_PATH = "/content/drive/MyDrive/ECG Arrhythmia Classification"

models_with_callbacks = {}
for name, model in models_dict.items():

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    callbacks = CALLBACKS_COMMON + [
        ModelCheckpoint(
            f"{DRIVE_PATH}/{name}_best.keras",
            monitor='val_loss',
            save_best_only=True
        )
    ]

    models_with_callbacks[name] = (model, callbacks)

histories = {}
for name, (m, cb) in models_with_callbacks.items():
    print(f"\n>>> Training {name}")
    hist = m.fit(
        X_train_c, y_train_cat,
        validation_data=(X_val_c, y_val_cat),
        epochs=100, batch_size=128,
        callbacks=cb, verbose=1
    )
    histories[name] = hist

def evaluate(m, X, y_cat):
    probs = m.predict(X, verbose=0)
    preds = probs.argmax(axis=1)
    truths= y_cat.argmax(axis=1)
    acc = (preds==truths).mean()
    conf= probs.max(axis=1).mean()
    return acc, conf

rows=[]
for name, (m,_) in models_with_callbacks.items():
    acc, conf = evaluate(m, X_val_c, y_val_cat)
    rows.append({"Model":name, "Accuracy":acc, "AvgConfidence":conf})
df = pd.DataFrame(rows).set_index("Model")
display(df)

plt.figure(figsize=(12,8))
for i,(name,h) in enumerate(histories.items()):
    plt.subplot(3,2,i*2+1)
    plt.plot(h.history['accuracy'],   label='train')
    plt.plot(h.history['val_accuracy'],label='val')
    plt.title(f"{name} Accuracy")
    plt.legend()
    plt.subplot(3,2,i*2+2)
    plt.plot(h.history['loss'],      label='train')
    plt.plot(h.history['val_loss'],  label='val')
    plt.title(f"{name} Loss")
    plt.legend()
plt.tight_layout()
plt.show()

"""# Transfer learning"""

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model
from sklearn.model_selection import train_test_split

TARGET_LEN = 188

def load_csv(path):
    df = pd.read_csv(path, header=None).values
    X = df[:, :-1]
    y = df[:, -1].astype(int)
    N, L = X.shape
    if L < TARGET_LEN:
        pad = TARGET_LEN - L
        X = np.pad(X, ((0,0),(0,pad)), 'constant')
    elif L > TARGET_LEN:
        X = X[:, :TARGET_LEN]
    X = X.reshape(N, TARGET_LEN, 1).astype('float32') / np.max(X)
    return X, y

data_dir = "/content/drive/MyDrive/ECG Arrhythmia Classification/archive 2"
X, y = load_csv(os.path.join(data_dir, "mitbih_train.csv"))
X_test, y_test = load_csv(os.path.join(data_dir, "mitbih_test.csv"))

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Shape train:", X_train.shape, y_train.shape)
print("Shape val:  ", X_val.shape,   y_val.shape)
print("Shape test: ", X_test.shape,  y_test.shape)

input_ecg = layers.Input(shape=(TARGET_LEN,1), name="ecg_input")
x = layers.Conv1D(32, 3, padding='same', activation='relu')(input_ecg)
x = layers.MaxPooling1D(2, padding='same')(x)
x = layers.Conv1D(16, 3, padding='same', activation='relu')(x)
encoded = layers.MaxPooling1D(2, padding='same', name="encoded")(x)

x = layers.Conv1D(16, 3, padding='same', activation='relu')(encoded)
x = layers.UpSampling1D(2)(x)
x = layers.Conv1D(32, 3, padding='same', activation='relu')(x)
x = layers.UpSampling1D(2)(x)
decoded = layers.Conv1D(1, 3, padding='same', activation='sigmoid', name="decoded")(x)

autoencoder = Model(input_ecg, decoded, name="ecg_autoencoder")
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.summary()

callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )
]

history = autoencoder.fit(
    X_train, X_train,
    epochs=100,
    batch_size=128,
    validation_data=(X_val, X_val),
    callbacks=callbacks,
    verbose=1
)

import matplotlib.pyplot as plt

plt.figure(figsize=(8,4))
plt.plot(history.history['loss'],   label='train loss')
plt.plot(history.history['val_loss'], label='val   loss')
plt.xlabel('Epoch')
plt.ylabel('MSE loss')
plt.legend()
plt.title('MSE trend during training')
plt.show()

n = 5
reconstructions = autoencoder.predict(X_val[:n])

plt.figure(figsize=(12, 2*n))
for i in range(n):

    ax = plt.subplot(n, 2, 2*i+1)
    plt.plot(X_val[i].squeeze(), color='C0')
    plt.title(f'Original  #{i}')
    plt.ylim(0,1)

    ax = plt.subplot(n, 2, 2*i+2)
    plt.plot(reconstructions[i].squeeze(), color='C1')
    plt.title(f'Reconstructed #{i}')
    plt.ylim(0,1)
plt.tight_layout()
plt.show()